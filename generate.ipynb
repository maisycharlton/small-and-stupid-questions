{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use anaconda3 python 3.9.12 kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maisy 2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = \"small and stupid questions\"\n",
    "max_length = 500 \n",
    "\n",
    "\n",
    "temperature = 1.2\n",
    "repetition_penalty = 1.5\n",
    "top_p = 0.92\n",
    "\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('./newmodel')\n",
    "#model = GPT2LMHeadModel.from_pretrained('./newmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: small and stupid questions is not going to stop people from writing these kind of things because they are too painful, it will just increase the damage that you cause,\" Krieger concluded. \"I don't want for them all this time...they're being told in every language why I need sex while we were living on our house with my partner at a young age.\"\n",
      "Text has been saved to 'generated_text.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure the tokenizer has the EOS token set as the padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    model.resize_token_embeddings(len(tokenizer))  # Resize model embeddings to account for new tokens\n",
    "\n",
    "# Set padding to the left\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "\n",
    "def generate_text(prompt, max_length=100, filename='generated_text.txt'):\n",
    "    if not prompt.strip():\n",
    "        return \"Please provide a valid, non-empty prompt.\"\n",
    "\n",
    "    # Tokenize the input prompt with left padding and truncation\n",
    "    encoded_input = tokenizer.encode_plus(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        padding='max_length',  # Ensures input is padded to max_length\n",
    "        truncation=True, \n",
    "        max_length=512  # Ensure this matches or exceeds the largest expected input\n",
    "    )\n",
    "\n",
    "    # Extract input_ids and attention_mask\n",
    "    input_ids = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "    # Set a higher max_length for more extended text generation\n",
    "    total_max_length = max_length + input_ids.size(1)  # Adding input length to desired output length\n",
    "\n",
    "    # Generate text from the model\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=total_max_length,  # Increased total maximum length\n",
    "        pad_token_id=tokenizer.pad_token_id,  # Use the pad token ID correctly\n",
    "        no_repeat_ngram_size=2,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        top_k=50\n",
    "        #stop_token=\".\"\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Save to file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(generated_text)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Generate and save text\n",
    "generated_text = generate_text(prompt, max_length)\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(\"Text has been saved to 'generated_text.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
