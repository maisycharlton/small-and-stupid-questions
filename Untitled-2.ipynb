{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maisy 2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Maisy 2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "with open('stupid_questions.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content as a single string\n",
    "    content = file.read().strip()  # strip() to remove any trailing newlines at the end of the file\n",
    "\n",
    "# Split the content by double newlines which separate sections\n",
    "texts = content.split('\\n\\n')\n",
    "\n",
    "# Tokenize and encode sequences\n",
    "encodings = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 72\n",
      "Evaluation dataset size: 7\n"
     ]
    }
   ],
   "source": [
    "# Assume encodings contain all the tokenized data\n",
    "total_data_size = len(encodings['input_ids'])\n",
    "# Define a minimum size for the evaluation dataset\n",
    "min_eval_size = max(2, int(0.1 * total_data_size))  # At least 10% or 1 sample for evaluation\n",
    "\n",
    "# Calculate the split index ensuring there's at least some eval data\n",
    "split_index = max(1, total_data_size - min_eval_size)\n",
    "\n",
    "# Split the dataset into training and evaluation\n",
    "train_encodings = {\n",
    "    'input_ids': encodings['input_ids'][:split_index],\n",
    "    'attention_mask': encodings['attention_mask'][:split_index]\n",
    "}\n",
    "eval_encodings = {\n",
    "    'input_ids': encodings['input_ids'][split_index:],\n",
    "    'attention_mask': encodings['attention_mask'][split_index:]\n",
    "}\n",
    "\n",
    "# Print out the sizes to confirm the split\n",
    "print(f\"Training dataset size: {len(train_encodings['input_ids'])}\")\n",
    "print(f\"Evaluation dataset size: {len(eval_encodings['input_ids'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = item['input_ids'].clone()  # Ensure labels are included for loss computation\n",
    "        return item\n",
    "\n",
    "# Example usage of the TextDataset\n",
    "train_dataset = TextDataset(train_encodings)  \n",
    "eval_dataset = TextDataset(eval_encodings)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator, DataLoaderConfiguration\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    dataloader_config=DataLoaderConfiguration(\n",
    "        dispatch_batches=None, \n",
    "        split_batches=False, \n",
    "        even_batches=True, \n",
    "        use_seedable_sampler=True\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maisy 2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pre-trained 'gpt2' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5400 [00:00<?, ?it/s]C:\\Users\\Maisy 2\\AppData\\Local\\Temp\\ipykernel_23504\\4093707277.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "  0%|          | 10/5400 [02:29<21:42:42, 14.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.6363, 'grad_norm': 152.97425842285156, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 10/5400 [02:34<21:42:42, 14.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.943297386169434, 'eval_runtime': 4.5152, 'eval_samples_per_second': 1.55, 'eval_steps_per_second': 0.221, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/5400 [03:03<23:18:27, 15.57s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Path to the output directory and specific checkpoint\n",
    "output_dir = r'C:\\Users\\Maisy 2\\Desktop\\small and stupid questions\\results'\n",
    "checkpoint_path = r'C:\\Users\\Maisy 2\\Desktop\\small and stupid questions\\results\\checkpoint'\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Load the model from the checkpoint if it exists, otherwise from 'gpt2'\n",
    "#if os.path.exists(checkpoint_path):\n",
    "    #model = GPT2LMHeadModel.from_pretrained(checkpoint_path)\n",
    "    #print(\"Loaded model from checkpoint:\", checkpoint_path)\n",
    "#else:\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "print(\"Loaded pre-trained 'gpt2' model\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,  # Ensure this matches the directory containing the checkpoints\n",
    "    num_train_epochs=600,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(output_dir, 'logs'),  # Logs can be in a subdirectory\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train(resume_from_checkpoint=checkpoint_path if os.path.exists(checkpoint_path) else None)\n",
    "\n",
    "# Save the model and tokenizer at the end of training\n",
    "model.save_pretrained(os.path.join(output_dir, 'newmodel'))\n",
    "tokenizer.save_pretrained(os.path.join(output_dir, 'newmodel'))\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
